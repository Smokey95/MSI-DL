{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed75fa4d",
   "metadata": {},
   "source": [
    "## 2. Training eines MLPs auf den Spielzeugdaten\n",
    "Der Code aus dem Beispielnotebook muss zunächst an das Szenario aus der Vorlesung angepasst werden: ein deutlich kleineres Netzwerk mit eindimensionalem statt zehndimensionalem Output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2673a67",
   "metadata": {},
   "source": [
    "Das folgende Beispiel beruht auf einer angepassten Version des TensorFlow Beispiel welches unten abgebildet ist bzw. [hier](https://playground.tensorflow.org./#activation=sigmoid&batchSize=10&dataset=xor&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=0&networkShape=2,2&seed=0.65710&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false) eigenständig durchgespielt werden kann. Die Netzwerkarchitektur, die Trainingsparameter und die Aktivierungsfunktion wurden wie in der folgenden Aufgabe angegeben festgelegt.\n",
    "\n",
    "![TensorFlow Experiment 2](./data/experiment_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c3402f",
   "metadata": {},
   "source": [
    "## 2.1 Hyperparameter und Netzarchitektur initialisieren\n",
    "Setzen Sie im Code die Größe der Minibatches auf 10, die Anzahl der Epochen auf 150 und die Lernrate auf 0.03. Ändern Sie die Netzarchitektur so ab, dass sie 2 Eingangsneuronen, 2 verdeckte Schichten mit jeweils 2 Neuronen und 1 Ausgangsneuron\n",
    "haben. Überprüfen Sie die Größen der sich daraus ergebenden Gewichtsmatritzen auf Korrektheit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac159e89",
   "metadata": {},
   "source": [
    "Zu beginn werden die Netzwerkparameter wie im Beispiel [training_eines_MLPLs_auf_MNIST](training_eines_MLPLs_auf_MNIST.ipynb) definiert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d0f3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Netzwerkparameter laut Aufgabenbeschreibung\n",
    "\n",
    "mbs = 10            # Größe des Mini-Batches\n",
    "eta = 0.03          # Lernrate eta\n",
    "epochs = 150        # Anzahl der Epochen\n",
    "no_hidden = 2       # Anzahl der Neuronen pro versteckter Schicht\n",
    "\n",
    "# Anzahl der Neuronen pro Schicht (2 Eingabe, 2 versteckte Schichten mit 2 Neuronen, 1 Ausgabe)\n",
    "layers = [2, no_hidden, no_hidden, 1]\n",
    "\n",
    "num_layers = len(layers)  # Anzahl der Schichten im Netzwerk\n",
    "num_layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52be8b74",
   "metadata": {},
   "source": [
    "Anschließend werden wie auch im Beispiel die Gewichte und Biases initialisiert und auf Korrektheit überprüft.\n",
    "\n",
    "Sowohl die Biase als auch die Gewichte werden dabei mit **zufälligen Werten initialisiert**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48298f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "biases = [np.random.randn(y, 1) for  y in layers[1:]]    # Schwellwerte\n",
    "weights = [np.random.randn(y, x) for x, y in zip(layers[:-1], layers[1:])]  # Gewichte\n",
    "\n",
    "print(\"Shape of biases:\")\n",
    "for b in biases:\n",
    "    print(b.shape)\n",
    "\n",
    "print(\"\\nShape of weights:\")\n",
    "for w in weights:\n",
    "    print(f\"{w.shape} (num weights: {w.size})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ebabd4b",
   "metadata": {},
   "source": [
    "**Erklärung Bias Shape:**\n",
    "Da jedes Neuron nur einen Bias hat, ist die Anzahl der Biases in einer Schicht gleich der Anzahl der Neuronen in dieser Schicht. Somit ergibt sich für jede Schicht mit $k (k \\geq 1)$ Neuronen ein Bias Array der Form $(L_k, 1)$ wobei $L_k$ die Anzahl der Neuronen in Schicht $k$ ist.\n",
    "\n",
    "**Erklärung Gewicht Shape:**\n",
    "Für die Gewichte zwischen Schicht $k-1$ und Schicht $k$ gilt: Gewichtsmatrix hat Form $(L_k, L_{k-1})$ (Zeilen = Neuronen der Zielschicht, Spalten = Neuronen der Quellschicht). Daher hat die letzte Gewichtsmatrix die Form (1, 2), da die Ausgabeschicht 1 Neuron und die vorherige Schicht 2 Neuronen hat.\n",
    "\n",
    "$\\rightarrow$ Die Eingangsschicht hat selbst weder Biases noch Gewichte, da sie nur die Eingabedaten weiterleitet.\n",
    "\n",
    "Durch die oben erstellte Konfiguration ergibt sich ein Netzwerk mit folgender Architektur:\n",
    "![Netzwerkarchitektur](./data/neural_network_exercise_full.drawio.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "900ee2be",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b335a5fd",
   "metadata": {},
   "source": [
    "## 2.2 Inklurdieren von Hilfsfunktionen und anpassen der Evaluierungsfunktion\n",
    "Der Beispielcode verwendet One-Hot-Coding für die Labels, in unserem Beispiel sind die Klassenzugehörigkeiten aber durch die Klassenindizes 0 und 1 codiert. Wir müssen daher die Funktion `evaluate()` im Code so abändern, dass ein Beispiel als korrekt klassifiziert gilt, wenn bei Klasse 0 der MLP-Output kleiner als 0.5 ist und bei Klasse 1 größer als 0.5. \n",
    "\n",
    "Berechnen Sie zusätzlich den $MSE$ in dieser Funktion bei jedem Aufruf und speichern Sie diesen in einem zusätzlichen Array ab."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43560768",
   "metadata": {},
   "source": [
    "Für die folgenden Aufgabenteile wird auf teilweise modifizierten Code aus dem Beispielnotebook [training_eines_MLPLs_auf_MNIST](training_eines_MLPLs_auf_MNIST.ipynb) zurückgegriffen.\n",
    "\n",
    "Im ersten Schritt muss die dort enthaltene Funktion `evaluate()` angepasst werden, um die Klassifikation entsprechend der neuen Anforderungen durchzuführen. Da diese jedoch von der Funktion welche den Vorwärtsdurchlauf durch das Netzwerk (`feedforward()`) abhängt muss zuerst diese sowie andere abhängige Funktionen inkludiert werden."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2cade24",
   "metadata": {},
   "source": [
    "### Inkludieren der Hilfsfunktionen\n",
    "\n",
    "Zuerst werden die notwendigen Hilfsfunktionen aus dem Beispielnotebook übernommen: `sigmoid()`, `sigmoid_prime()`, `cost_derivative()`. Da diese lediglich elementare mathematische Operationen durchführen, müssen diese nicht angepasst werden.\n",
    "\n",
    "Für diesen Aufgabenteil wird wie in der zweiten Vorlesung (02_backprop.pdf) auf S.22 die **Sigmoid-Aktivierungsfunktion** verwendet:\n",
    "$$\n",
    "\\sigma(z^l) = \\frac{1}{1 + e^{-z^{l}}}\n",
    "$$\n",
    "mit der Ableitung:\n",
    "$$\n",
    "\\sigma'(z^l) = \\sigma(z^l) \\cdot (1 - \\sigma(z^l))\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb6d5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sigmoid (vektorisiert)\n",
    "def sigmoid(z):\n",
    "    \"\"\"The sigmoid function.\"\"\"\n",
    "    return 1.0/(1.0+np.exp(-z))\n",
    "\n",
    "# Ableitung des Sigmoids\n",
    "def sigmoid_prime(z):\n",
    "    \"\"\"Derivative of the sigmoid function.\"\"\"\n",
    "    return sigmoid(z)*(1-sigmoid(z))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e68b0a",
   "metadata": {},
   "source": [
    "Zusätzlich soll der $MSE$ in dieser Funktion bei jedem Aufruf berechnet und in einem zusätzlichen Array abgespeichert werden. Der **Mean Squared Error (MSE)** wird wie in der Vorlesung wie folgt definiert:\n",
    "$$\n",
    "C(a^L) = \\frac{1}{2} \\sum_k (y_k - a^L_k)^2\n",
    "$$\n",
    "mit dem **Gradienten**:\n",
    "$$\n",
    "\\nabla_{a^L}C = \\frac{\\partial C}{\\partial a^L} = a^L - y\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880b16a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ableitung der MSE-Kostenfunktion\n",
    "def cost_derivative(output_activations, y):\n",
    "    \"\"\"Return the vector of partial derivatives ∂C/∂a for the output activations.\"\"\"\n",
    "    return (output_activations-y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1eaee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MSE-Kostenfunktion\n",
    "def cost(output_activations, y):\n",
    "    \"\"\"\n",
    "    Return the MSE cost.\n",
    "    #### Arguments:\n",
    "        output_activations -- The output activations from the network\n",
    "        y -- The true labels\n",
    "    #### Returns:\n",
    "        The MSE cost as a float value\n",
    "    \"\"\"\n",
    "    return (0.5 * np.linalg.norm(output_activations - y) ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c898df77",
   "metadata": {},
   "source": [
    "### Feedforward-Funktion\n",
    "\n",
    "Anschließend wird die `feedforward()`-Funktion aus dem Beispielnotebook übernommen, welche den Vorwärtsdurchlauf durch das Netzwerk implementiert. Auch diese Funktion muss nicht angepasst werden, da sie lediglich die Aktivierungen der Neuronen berechnet.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c6cce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feedforward(a):\n",
    "    \"\"\"Return the output of the network if ``a`` is input.\"\"\"\n",
    "    for b, w in zip(biases, weights):\n",
    "        a = sigmoid(np.dot(w, a)+b)\n",
    "    return a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a52d4133",
   "metadata": {},
   "source": [
    "### Evaluierungsfunktion anpassen\n",
    "\n",
    "Nun kann die `evaluate()`-Funktion angepasst werden, um die Klassifikation entsprechend der neuen Anforderungen durchzuführen. Zusätzlich wird der $MSE$ berechnet und in einem Array abgespeichert.\n",
    "\n",
    "Da es sich bei der Aktivierungsfunktion um eine Sigmoidfunktion handelt die nur Werte im Intervall $(0,1)$ annimmt, wird ein Schwellenwert von $0.5$ verwendet um die Klassenzugehörigkeit zu bestimmen. \n",
    "\n",
    "Ist die Vorhersage des Netzwerks für ein Beispiel größer oder gleich 0.5, so wird das Beispiel der Klasse 1 zugeordnet, andernfalls der Klasse 0. Ein Beispiel gilt als korrekt klassifiziert, wenn die vorhergesagte Klasse (`cpred`) mit dem tatsächlichen Label (`c` bzw. `y`) übereinstimmt (`c == ypred`S)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5368c520",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(x2, y2):\n",
    "    \"\"\"Return the number of test inputs for which the neural\n",
    "    network outputs the correct result. Note that the neural\n",
    "    network's output is assumed to be the index of whichever\n",
    "    neuron in the final layer has the highest activation.\"\"\"\n",
    "    \n",
    "    correct = 0 # Anzahl korrekt klassifizierter Testbeispiele\n",
    "    loss = []   # Liste zur Speicherung der Verluste (Erweiterung laut Aufgabenbeschreibung)\n",
    "    \n",
    "    # Gehe den Testdatensatz durch\n",
    "    for i in range(0, x2.shape[0]):\n",
    "        # Eingabe- und Ausgabedaten extrahieren\n",
    "        x = np.reshape(x2[i,:],(x2.shape[1],1)).copy()\n",
    "        if len(y2.shape) == 2:\n",
    "            y = np.reshape(y2[i,:],(y2.shape[1],1)).copy()\n",
    "        else:\n",
    "            y = y2[i].copy()\n",
    "        \n",
    "        # Vorwärtslauf\n",
    "        ypred = feedforward(x)\n",
    "        \n",
    "        # Die Vorhersage ist 1, wenn die Ausgabe > 0.5 ist, sonst 0\n",
    "        cpred = 1 if ypred >= 0.5 else 0\n",
    "\n",
    "        # Das tatsächliche Label\n",
    "        c = y\n",
    "        \n",
    "        # Falls beide übereinstimmen, addiere zur Gesamtzahl\n",
    "        if c == cpred:\n",
    "            correct += 1\n",
    "\n",
    "        # Loss berechnen und speichern (Erweiterung laut Aufgabenbeschreibung)\n",
    "        loss.append(cost(ypred, y))\n",
    "        \n",
    "    return correct, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d992cd",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abecae8a",
   "metadata": {},
   "source": [
    "## 2.3 Netz trainieren und Ergebnisse vergleichen\n",
    "Trainieren Sie Ihr Netz mit den Trainingsdaten als Validierungsdaten und testen Sie es auf Ihren Testdaten. Stellen Sie die Lernkurven für Genauigkeit und MSE als Plots dar. Beachten Sie hierbei, dass unser Lernproblem nicht konvex ist, so dass die Optimierung zuweilen in lokalen Minima hängenbleiben kann. Wiederholen Sie Ihren Versuch daher mehrere Male und vergeleichen Sie die Ergebnisse."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a71860",
   "metadata": {},
   "source": [
    "### Datensätze generieren\n",
    "\n",
    "Wie in Aufgabe 1 [exercise_1_1](exercise_1_1.ipynb) gezeigt müssen zuerst die Daten auch für diese Aufgabe wieder entsprechend generiert werden. Hierzu werden wie auch in der vorherigen Aufgabe beschrieben zum erstellen der Labels und Design-Matrizen die Funktionen `get_labels()` und `build_design_matrix()` aus der Datei [`auxiliary.py`](auxiliary.py) verwendet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a763c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from auxiliary import get_labels, build_design_matrix\n",
    "\n",
    "# Erstellen der Datenpunkte im vorgegebenen Bereich für Training und Test\n",
    "X_train = np.random.uniform(-6, 6, (200, 2))\n",
    "X_test = np.random.uniform(-6, 6, (200, 2))\n",
    "\n",
    "# Labels für Trainings- und Testdaten generieren\n",
    "y_train = get_labels(X_train)\n",
    "y_test = get_labels(X_test)\n",
    "\n",
    "# Gewichtungsvektoren für x- und y-Richtung\n",
    "w_x = np.array([1, 0])\n",
    "w_y = np.array([0, 1])\n",
    "\n",
    "# Design Matrix für das Gitter erstellen\n",
    "xv = np.linspace(-6, 6, 100)\n",
    "yv = np.linspace(-6, 6, 100)\n",
    "X, Y = np.meshgrid(xv, yv)\n",
    "\n",
    "design = np.c_[X.ravel(), Y.ravel()]\n",
    "\n",
    "# Design-Matrizen für Trainings- und Testdaten erstellen sowie für das Gitter\n",
    "design_X_train = build_design_matrix(X_train, w_x, w_y)\n",
    "design_X_test = build_design_matrix(X_test, w_x, w_y)\n",
    "design_grid = build_design_matrix(design, w_x, w_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e95bc9a",
   "metadata": {},
   "source": [
    "### Backpropagation-Algorithmus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb0ef35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backprop(x, y):\n",
    "    \"\"\"Return a tuple ``(nabla_b, nabla_w)`` representing the\n",
    "    gradient for the cost function C_x.  ``nabla_b`` and\n",
    "    ``nabla_w`` are layer-by-layer lists of numpy arrays, similar\n",
    "    to ``self.biases`` and ``self.weights``.\"\"\"\n",
    "    \n",
    "    # Initialisiere Updates für Schwellwerte und Gewichte\n",
    "    nabla_b = [np.zeros(b.shape) for b in biases]\n",
    "    nabla_w = [np.zeros(w.shape) for w in weights]\n",
    "    \n",
    "    # Vorwärtslauf\n",
    "    activation = x # Initialisierung a^1 = x\n",
    "    activations = [x] # list to store all the activations, layer by layer\n",
    "    zs = [] # list to store all the z vectors, layer by layer\n",
    "    for b, w in zip(biases, weights):\n",
    "        z = np.dot(w, activation) + b\n",
    "        zs.append(z)\n",
    "        activation = sigmoid(z)\n",
    "        activations.append(activation)\n",
    "    \n",
    "    # Rückwärtslauf\n",
    "    delta = cost_derivative(activations[-1], y) * sigmoid_prime(zs[-1]) # Fehler am Output\n",
    "    nabla_b[-1] = delta # Update Schwellwert in der Ausgangsschicht\n",
    "    nabla_w[-1] = np.dot(delta, activations[-2].transpose()) # Update Gewichte in der Ausgangsschicht\n",
    "    for l in range(2, num_layers): # Backpropagation\n",
    "        z = zs[-l] # gewichteter Input\n",
    "        sp = sigmoid_prime(z) # Ableitung der Aktivierungsfunktion\n",
    "        delta = np.dot(weights[-l+1].transpose(), delta) * sp # Fehler in Schicht l\n",
    "        nabla_b[-l] = delta # Update Schwellwert \n",
    "        nabla_w[-l] = np.dot(delta, activations[-l-1].transpose()) # Update Gewichte\n",
    "\n",
    "    return (nabla_b, nabla_w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee526ea",
   "metadata": {},
   "source": [
    "### Minibatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dfe6881",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_mini_batch(xmb, ymb, eta):\n",
    "    \"\"\"Update the network's weights and biases by applying\n",
    "    gradient descent using backpropagation to a single mini batch.\n",
    "    The ``mini_batch`` is a list of tuples ``(x, y)``, and ``eta``\n",
    "    is the learning rate.\"\"\"\n",
    "    global weights\n",
    "    global biases\n",
    "\n",
    "    # Initialisiere Updates für Schwellwerte und Gewichte\n",
    "    nabla_b = [np.zeros(b.shape) for b in biases]\n",
    "    nabla_w = [np.zeros(w.shape) for w in weights]\n",
    "    \n",
    "    # Gehe durch alle Beispielpaare im Minibatch\n",
    "    for i in range(xmb.shape[0]):\n",
    "        x = np.reshape(xmb[i,:],(xmb.shape[1],1)).copy()\n",
    "        if len(ymb.shape) == 2:\n",
    "            y = np.reshape(ymb[i,:],(ymb.shape[1],1)).copy()\n",
    "        else:\n",
    "            y = ymb[i].copy()\n",
    "        \n",
    "        # Berechne Updates für alle Schichten über Backprop\n",
    "        delta_nabla_b, delta_nabla_w = backprop(x, y)\n",
    "        \n",
    "        # Addiere einzelne Updates auf\n",
    "        nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
    "        nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
    "    \n",
    "    # Berechne neue Gewichte\n",
    "    weights = [w-(eta/xmb.shape[0])*nw\n",
    "                    for w, nw in zip(weights, nabla_w)]\n",
    "    biases = [b-(eta/xmb.shape[0])*nb\n",
    "                   for b, nb in zip(biases, nabla_b)]\n",
    "    \n",
    "    #print(\"shape weights:\")\n",
    "    #for w in weights:\n",
    "    #        print(f\"{w.shape}\")\n",
    "\n",
    "    return (weights, biases)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ad35f7",
   "metadata": {},
   "source": [
    "### Stochastischer Gradientenabstieg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ee87ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SGD(x0, y0, epochs, mini_batch_size, eta, x2, y2):\n",
    "\n",
    "    n_test = x2.shape[0] # Anzahl Testdaten\n",
    "    n = x0.shape[0]      # Anzahl Trainingsdaten\n",
    "    \n",
    "    precision_curve =   []  # Liste zur Speicherung der Präzisionswerte (Erweiterung laut Aufgabenbeschreibung)\n",
    "    mse_curve =         []  # Liste zur Speicherung der MSE-Werte (Erweiterung laut Aufgabenbeschreibung)\n",
    "\n",
    "    # gehe durch alle Epochen\n",
    "    acc_val = np.zeros(epochs)\n",
    "\n",
    "    print(\"| Epochs | Precision | Loss   |\")\n",
    "    for j in range(epochs):\n",
    "        \n",
    "        # Bringe die Trainingsdaten in eine zufällige Reihenfolge für jede Epoche\n",
    "        p = np.random.permutation(n) # Zufällige Permutation aller Indizes von 0 .. n-1\n",
    "        x0 = x0[p,:]\n",
    "        y0 = y0[p]\n",
    "        \n",
    "        # Zerlege den permutierten Datensatz in Minibatches \n",
    "        for k in range(0, n, mini_batch_size):\n",
    "            xmb = x0[k:k+mini_batch_size,:]\n",
    "            if len(y0.shape) == 2:\n",
    "                ymb = y0[k:k+mini_batch_size,:]\n",
    "            else:\n",
    "                ymb = y0[k:k+mini_batch_size]\n",
    "            update_mini_batch(xmb, ymb, eta)\n",
    "        \n",
    "        # Gib Performance aus\n",
    "        acc_val[j], loss = evaluate(x2, y2)\n",
    "\n",
    "        precision_curve.append(acc_val[j] / n_test)  # Präzisionswert speichern\n",
    "        mse_curve.append(np.mean(loss))              # MSE-Wert speichern\n",
    "\n",
    "        if j % 10 == 0 or j == epochs - 1:\n",
    "            print(\"|  {:>5} |   {:>6.4f}  | {:>6.4f} |\".format(j+1, precision_curve[-1], mse_curve[-1]))\n",
    "    \n",
    "    return acc_val, precision_curve, mse_curve"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ffb92db",
   "metadata": {},
   "source": [
    "### Training des Netzwerks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c210977d",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_val, precision, loss = SGD(design_X_train, y_train, epochs, mbs, eta, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f0d931e",
   "metadata": {},
   "source": [
    "### Darstellen der Lernkurven"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161c00a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "### Darstellen der Lernkurven\n",
    "# Genauigkeit-Kurve\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(epochs), precision, label='Genauigkeit', color='blue')\n",
    "plt.xlabel('Epochen')\n",
    "plt.ylabel('Genauigkeit')\n",
    "plt.title('Genauigkeit-Kurve')\n",
    "plt.grid()\n",
    "# MSE-Kurve\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(epochs), loss, label='MSE', color='red')\n",
    "plt.xlabel('Epochen')\n",
    "plt.ylabel('Mean Squared Error (MSE)')\n",
    "plt.title('MSE-Kurve')\n",
    "plt.grid()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2432db40",
   "metadata": {},
   "source": [
    "Die MSE und Genauigkeit werden während des Trainings in Listen gespeichert und anschließend geplottet, um die Lernkurven darzustellen. Hierbei sind je nach Durchlauf aber nur geringe Verbesserungen zu erkennen, da das Netzwerk evtl. in lokalen Minima hängen bleibt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e8e839",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "495eaeac",
   "metadata": {},
   "source": [
    "## 2.4 Entscheidungsfunktion visualisieren\n",
    "Berechnen Sie die Entscheidungsfunktion Ihres MLPs für Ihr 100 × 100-Gitter und stellen Sie diese gemeinsam mit dem Scatterplot Ihrer Trainingsdaten dar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "689e3f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_design_decision = feedforward(design_grid.T)\n",
    "\n",
    "figure, axis = plt.subplots(1, 1, figsize=(6, 6), dpi=80)\n",
    "axis_limits = [-6, 6]\n",
    "\n",
    "# Plot der Entscheidungsfunktion\n",
    "axis.pcolor(X, Y, mlp_design_decision.reshape(100,100), alpha=.8)\n",
    "axis.grid(True)\n",
    "axis.set_title(\"Entscheidungsfunktion 100 x 100 Gitter und Trainingsdaten\")\n",
    "axis.set_xlabel(\"x\")\n",
    "axis.set_ylabel(\"y\")\n",
    "axis.axhline(0, color='black', linewidth=0.5)\n",
    "axis.axvline(0, color='black', linewidth=0.5)\n",
    "axis.set_xlim(axis_limits)\n",
    "axis.set_ylim(axis_limits)\n",
    "axis.set_aspect('equal')\n",
    "\n",
    "# Scatter-Plot der Labels\n",
    "plt.scatter(X_train[y_train == 1, 0], X_train[y_train == 1, 1], color='blue', label='Label 1')\n",
    "plt.scatter(X_train[y_train == 0, 0], X_train[y_train == 0, 1], color='red', label='Label 0')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
