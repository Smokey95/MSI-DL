{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c197c67",
   "metadata": {},
   "source": [
    "# Exercise 2.1: Binary Classification with JAX and Keras\n",
    "\n",
    "In diesem Notebook sollten Sie die logistische Regression aus dem ersten Notebook nachbilden, diesmal jedoch das Autodiff-Framework von JAX und Keras verwenden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16579a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import platform\n",
    "import subprocess\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# --- JAX ---\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "# --- NumPy ---\n",
    "import numpy as np\n",
    "\n",
    "# --- PyTorch ---\n",
    "import torch\n",
    "\n",
    "# --- Keras 3 ---\n",
    "os.environ[\"KERAS_BACKEND\"] = \"torch\"   # or \"jax\", depending on usage\n",
    "import keras\n",
    "\n",
    "print(\"=== Python & OS Info ===\")\n",
    "print(\"Date: {}\".format(datetime.now()))\n",
    "print(f\"Python version:      {platform.python_version()}\")\n",
    "print(f\"System:              {platform.system()}\")\n",
    "print(f\"Machine:             {platform.machine()}\")             # arm64 on Apple Silicon\n",
    "print(f\"Processor:           {platform.processor()}\")\n",
    "print(f\"Mac version:         {platform.mac_ver()[0]}\")           # macOS version\n",
    "print()\n",
    "# Optionally: Detect Apple chip\n",
    "def get_chip_info():\n",
    "    try:\n",
    "        out = subprocess.check_output(\n",
    "            [\"sysctl\", \"-n\", \"machdep.cpu.brand_string\"]\n",
    "        ).decode().strip()\n",
    "        return out\n",
    "    except Exception:\n",
    "        return \"unknown\"\n",
    "\n",
    "print(f\"Chip:          {get_chip_info()}\")\n",
    "print()\n",
    "\n",
    "print(\"=== JAX Info ===\")\n",
    "print(f\"JAX version:         {jax.__version__}\")\n",
    "print(f\"JAX devices:         {jax.devices()}\")\n",
    "print()\n",
    "\n",
    "print(\"=== PyTorch Info ===\")\n",
    "print(f\"PyTorch version:     {torch.__version__}\")\n",
    "print(f\"CUDA available:      {torch.cuda.is_available()}\")\n",
    "print(f\"Device:              {torch.device('cuda' if torch.cuda.is_available() else 'cpu')}\")\n",
    "print()\n",
    "\n",
    "print(\"=== NumPy & Keras ===\")\n",
    "print(f\"NumPy version:       {np.__version__}\")\n",
    "print(f\"Keras version:       {keras.__version__}\")\n",
    "print(f\"Keras backend:       {keras.config.backend()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "263fe704",
   "metadata": {},
   "source": [
    "### Generate training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca70a8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def generate_training_data(ntrain=200, scale=6.0, seed=None):\n",
    "    \"\"\"\n",
    "    Reproduce the training data (x2, y0) from 01_mlp.ipynb.\n",
    "\n",
    "    - 2D inputs start uniform on [0, scale]^2\n",
    "    - Then mapped into four quadrants\n",
    "    - Labels: quadrants I & III -> 1, quadrants II & IV -> 0\n",
    "    - Final features x2 are sigmoid-transformed coordinates (perzeptron with w = I, b = 0)\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "\n",
    "    # 2d-Inputs (this is x0 in the notebook)\n",
    "    x0 = rng.random((ntrain, 2))\n",
    "    x0 *= scale\n",
    "\n",
    "    # Quadrant manipulation exactly as in the notebook\n",
    "    q = ntrain // 4\n",
    "\n",
    "    # indices [q : 2q): flip x -> quadrant II\n",
    "    x0[q:2*q, 0] *= -1.0\n",
    "\n",
    "    # indices [2q : 3q): flip y -> quadrant IV\n",
    "    x0[2*q:3*q, 1] *= -1.0\n",
    "\n",
    "    # indices [3q : ntrain): flip both -> quadrant III\n",
    "    x0[3*q:, :] *= -1.0\n",
    "\n",
    "    # labels (y0 in the notebook)\n",
    "    y0 = np.ones(ntrain, dtype=float)\n",
    "    y0[q:2*q] = 0.0        # quadrant II\n",
    "    y0[2*q:3*q] = 0.0      # quadrant IV\n",
    "    # first q and last q remain 1.0 (quadrants I & III)\n",
    "\n",
    "    # Perzeptron transform with w = I, b = 0 -> this is x2 in the notebook\n",
    "    def sigmoid(z):\n",
    "        return 1.0 / (1.0 + np.exp(-z))\n",
    "\n",
    "    x2 = sigmoid(x0)\n",
    "\n",
    "    return x2, y0\n",
    "\n",
    "# In the first notebook a scale of 6.0 was used, here we use 1.0\n",
    "x2, y0 = generate_training_data(ntrain=200, scale=1.0, seed=42) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404e841d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_data_and_probability(x, y, params=None, predict_proba_func=None, figsize=(8, 6)):\n",
    "    \"\"\"\n",
    "    Plot training data and optionally forecasted probability.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    x : array-like, shape (n_samples, 2)\n",
    "        Training data features\n",
    "    y : array-like, shape (n_samples,)\n",
    "        Training data labels (0 or 1)\n",
    "    params : dict, optional\n",
    "        Model parameters. If None, only data is plotted.\n",
    "    predict_proba_func : callable, optional\n",
    "        Function to predict probabilities: predict_proba_func(params, x)\n",
    "        Required if params is not None.\n",
    "    figsize : tuple, optional\n",
    "        Figure size (default: (8, 6))\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "\n",
    "    # Create a grid for probability contours (if model is provided)\n",
    "    if params is not None and predict_proba_func is not None:\n",
    "        x1_min, x1_max = x[:, 0].min() - 0.1, x[:, 0].max() + 0.1\n",
    "        x2_min, x2_max = x[:, 1].min() - 0.1, x[:, 1].max() + 0.1\n",
    "        xx1, xx2 = np.meshgrid(np.linspace(x1_min, x1_max, 100),\n",
    "                               np.linspace(x2_min, x2_max, 100))\n",
    "        grid_points = np.c_[xx1.ravel(), xx2.ravel()]\n",
    "\n",
    "        # Predict probabilities on the grid\n",
    "        proba_grid = predict_proba_func(params, grid_points)\n",
    "        proba_grid = np.array(proba_grid).reshape(xx1.shape)\n",
    "\n",
    "        # Plot probability contours (reversed colormap: red = high prob for class 1)\n",
    "        contour = ax.contourf(xx1, xx2, proba_grid, levels=20, cmap='RdYlBu_r', alpha=0.6)\n",
    "        cbar1 = plt.colorbar(contour, ax=ax)\n",
    "        cbar1.set_label('Forecasted probability p(y=1|x)', fontsize=10)\n",
    "\n",
    "        # Add decision boundary (p=0.5)\n",
    "        contour_lines = ax.contour(xx1, xx2, proba_grid, levels=[0.5], colors='black', \n",
    "                                   linewidths=2, linestyles='--', zorder=5)\n",
    "        ax.clabel(contour_lines, inline=True, fontsize=10, fmt='p=0.5')\n",
    "        \n",
    "        title = 'Training Data and Forecasted Probability'\n",
    "    else:\n",
    "        title = 'Training data in feature space x2'\n",
    "\n",
    "    # Plot training data points\n",
    "    scatter = ax.scatter(x[:, 0], x[:, 1], c=y, cmap='bwr', \n",
    "                         edgecolor='k', linewidth=1.5, s=50, alpha=0.9, zorder=10)\n",
    "    cbar2 = plt.colorbar(scatter, ax=ax, ticks=[0, 1], pad=0.15)\n",
    "    cbar2.set_label('True label y0', fontsize=10)\n",
    "\n",
    "    ax.set_xlabel('x1 (after sigmoid)', fontsize=11)\n",
    "    ax.set_ylabel('x2 (after sigmoid)', fontsize=11)\n",
    "    ax.set_title(title, fontsize=12, fontweight='bold')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot the training data\n",
    "plot_data_and_probability(x2, y0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa89f560",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248c5d08",
   "metadata": {},
   "source": [
    "### 2.1.1 Logistische Regression mit JAX\n",
    "\n",
    "Bitte implementieren Sie das logistische Regressionsmodell und die negative Log-Likelihood-Verlustfunktion mit JAX. Stellen Sie sich JAX als Ersatz für NumPy mit automatischer Differentiation vor. Sie können den folgenden Code als Ausgangspunkt verwenden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd4b4d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# Parameter initialization (fully implemented)\n",
    "# ------------------------------------------------------------\n",
    "def init_params(seed=0):\n",
    "    \"\"\"\n",
    "    Initialize parameters for logistic regression.\n",
    "\n",
    "    Returns a dictionary:\n",
    "        params = {\n",
    "            \"w\": w,   # shape (2,)\n",
    "            \"b\": b    # scalar\n",
    "        }\n",
    "    \"\"\"\n",
    "    key = jax.random.PRNGKey(seed)\n",
    "    # Random initialization for weights (small values)\n",
    "    w = 0.1 * jax.random.normal(key, (2,))\n",
    "    # Bias initialized to zero\n",
    "    b = jnp.array(0.0)\n",
    "    return {\"w\": w, \"b\": b}\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Model: p(y = 1 | x) given the parameters\n",
    "# ------------------------------------------------------------\n",
    "def predict_proba(params, x):\n",
    "    \"\"\"\n",
    "    Compute p(y = 1 | x).\n",
    "\n",
    "    TODO:\n",
    "      - Return probability p for class 1\n",
    "    \"\"\"\n",
    "    w = params[\"w\"]\n",
    "    b = params[\"b\"]\n",
    "    # return p\n",
    "    raise NotImplementedError(\"predict_proba() is not implemented yet.\")\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Negative Log-Likelihood loss\n",
    "# ------------------------------------------------------------\n",
    "def nll_loss(params, x, y):\n",
    "    \"\"\"\n",
    "    Binary cross-entropy / negative log-likelihood.\n",
    "\n",
    "    TODO:\n",
    "      - Return negative mean of the negative log-likelihood\n",
    "    \"\"\"\n",
    "    raise NotImplementedError(\"nll_loss() is not implemented yet.\")\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Full-batch gradient descent training loop\n",
    "# ------------------------------------------------------------\n",
    "def train(x, y, lr=0.5, n_steps=2000):\n",
    "    \"\"\"\n",
    "    Full-batch gradient descent training.\n",
    "\n",
    "    TODO:\n",
    "      - Initialize params = init_params()\n",
    "      - Define grad_loss = jax.grad(nll_loss)\n",
    "      - Loop over steps:\n",
    "          * compute gradients\n",
    "          * update w and b\n",
    "      - Print loss every 200 steps (optional)\n",
    "      - Return final params\n",
    "    \"\"\"\n",
    "    params = init_params()\n",
    "    grad_loss = jax.grad(nll_loss)\n",
    "    for step in range(n_steps):\n",
    "        grads = grad_loss(params, x, y)\n",
    "    raise NotImplementedError(\"train() is not implemented yet.\")\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# After students implement everything:\n",
    "# params = train(x2, y0)\n",
    "# ------------------------------------------------------------\n",
    "# Plot the data and forecasted probability\n",
    "# plot_data_and_probability(x2, y0, params, predict_proba)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7577a623",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f471d0",
   "metadata": {},
   "source": [
    "### 2.1.2 Multilayer Perceptron\n",
    "\n",
    "Fügen Sie dem Modell eine versteckte Ebene der Dimension 16 hinzu und wiederholen Sie [2.1.1](#211-logistische-regression-mit-jax). \n",
    "\n",
    "Um die Leistung zu verbessern, verwenden Sie jit, indem Sie den Aktualisierungsschritt in eine jit-Funktion einfügen.\n",
    "\n",
    "```python\n",
    "@jax.jit\n",
    "def update(params, x, y, lr):\n",
    "    grads = jax.grad(nll_loss)(params, x, y)\n",
    "    return {\n",
    "        \"W1\": params[\"W1\"] - lr * grads[\"W1\"],\n",
    "        # YOUR CODE HERE\n",
    "        }\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16399b60",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a6dcef0",
   "metadata": {},
   "source": [
    "### 2.1.3 Implementieren Sie das MLP in Keras.\n",
    "\n",
    "Implementieren Sie das MLP in Keras unter Verwendung der sequenziellen API."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
