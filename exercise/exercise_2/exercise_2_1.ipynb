{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c197c67",
   "metadata": {},
   "source": [
    "# Exercise 2.1: Binary Classification with JAX and Keras\n",
    "\n",
    "In diesem Notebook sollten Sie die logistische Regression aus dem ersten Notebook nachbilden, diesmal jedoch das Autodiff-Framework von JAX und Keras verwenden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16579a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import platform\n",
    "import subprocess\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# --- JAX ---\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "# --- NumPy ---\n",
    "import numpy as np\n",
    "\n",
    "# --- PyTorch ---\n",
    "import torch\n",
    "\n",
    "# --- Keras 3 ---\n",
    "os.environ[\"KERAS_BACKEND\"] = \"torch\"   # or \"jax\", depending on usage\n",
    "import keras\n",
    "\n",
    "print(\"=== Python & OS Info ===\")\n",
    "print(\"Date: {}\".format(datetime.now()))\n",
    "print(f\"Python version:      {platform.python_version()}\")\n",
    "print(f\"System:              {platform.system()}\")\n",
    "print(f\"Machine:             {platform.machine()}\")             # arm64 on Apple Silicon\n",
    "print(f\"Processor:           {platform.processor()}\")\n",
    "print(f\"Mac version:         {platform.mac_ver()[0]}\")           # macOS version\n",
    "print()\n",
    "# Optionally: Detect Apple chip\n",
    "def get_chip_info():\n",
    "    try:\n",
    "        out = subprocess.check_output(\n",
    "            [\"sysctl\", \"-n\", \"machdep.cpu.brand_string\"]\n",
    "        ).decode().strip()\n",
    "        return out\n",
    "    except Exception:\n",
    "        return \"unknown\"\n",
    "\n",
    "print(f\"Chip:          {get_chip_info()}\")\n",
    "print()\n",
    "\n",
    "print(\"=== JAX Info ===\")\n",
    "print(f\"JAX version:         {jax.__version__}\")\n",
    "print(f\"JAX devices:         {jax.devices()}\")\n",
    "print()\n",
    "\n",
    "print(\"=== PyTorch Info ===\")\n",
    "print(f\"PyTorch version:     {torch.__version__}\")\n",
    "print(f\"CUDA available:      {torch.cuda.is_available()}\")\n",
    "print(f\"Device:              {torch.device('cuda' if torch.cuda.is_available() else 'cpu')}\")\n",
    "print()\n",
    "\n",
    "print(\"=== NumPy & Keras ===\")\n",
    "print(f\"NumPy version:       {np.__version__}\")\n",
    "print(f\"Keras version:       {keras.__version__}\")\n",
    "print(f\"Keras backend:       {keras.config.backend()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "263fe704",
   "metadata": {},
   "source": [
    "### Generate training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca70a8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def generate_training_data(ntrain=200, scale=6.0, seed=None):\n",
    "    \"\"\"\n",
    "    Reproduce the training data (x2, y0) from 01_mlp.ipynb.\n",
    "\n",
    "    - 2D inputs start uniform on [0, scale]^2\n",
    "    - Then mapped into four quadrants\n",
    "    - Labels: quadrants I & III -> 1, quadrants II & IV -> 0\n",
    "    - Final features x2 are sigmoid-transformed coordinates (perzeptron with w = I, b = 0)\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "\n",
    "    # 2d-Inputs (this is x0 in the notebook)\n",
    "    x0 = rng.random((ntrain, 2))\n",
    "    x0 *= scale\n",
    "\n",
    "    # Quadrant manipulation exactly as in the notebook\n",
    "    q = ntrain // 4\n",
    "\n",
    "    # indices [q : 2q): flip x -> quadrant II\n",
    "    x0[q:2*q, 0] *= -1.0\n",
    "\n",
    "    # indices [2q : 3q): flip y -> quadrant IV\n",
    "    x0[2*q:3*q, 1] *= -1.0\n",
    "\n",
    "    # indices [3q : ntrain): flip both -> quadrant III\n",
    "    x0[3*q:, :] *= -1.0\n",
    "\n",
    "    # labels (y0 in the notebook)\n",
    "    y0 = np.ones(ntrain, dtype=float)\n",
    "    y0[q:2*q] = 0.0        # quadrant II\n",
    "    y0[2*q:3*q] = 0.0      # quadrant IV\n",
    "    # first q and last q remain 1.0 (quadrants I & III)\n",
    "\n",
    "    # Perzeptron transform with w = I, b = 0 -> this is x2 in the notebook\n",
    "    def sigmoid(z):\n",
    "        return 1.0 / (1.0 + np.exp(-z))\n",
    "\n",
    "    x2 = sigmoid(x0)\n",
    "\n",
    "    return x2, y0\n",
    "\n",
    "# In the first notebook a scale of 6.0 was used, here we use 1.0\n",
    "x2, y0 = generate_training_data(ntrain=200, scale=1.0, seed=42) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404e841d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_data_and_probability(x, y, params=None, predict_proba_func=None, figsize=(8, 6)):\n",
    "    \"\"\"\n",
    "    Plot training data and optionally forecasted probability.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    x : array-like, shape (n_samples, 2)\n",
    "        Training data features\n",
    "    y : array-like, shape (n_samples,)\n",
    "        Training data labels (0 or 1)\n",
    "    params : dict, optional\n",
    "        Model parameters. If None, only data is plotted.\n",
    "    predict_proba_func : callable, optional\n",
    "        Function to predict probabilities: predict_proba_func(params, x)\n",
    "        Required if params is not None.\n",
    "    figsize : tuple, optional\n",
    "        Figure size (default: (8, 6))\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "\n",
    "    # Create a grid for probability contours (if model is provided)\n",
    "    if params is not None and predict_proba_func is not None:\n",
    "        x1_min, x1_max = x[:, 0].min() - 0.1, x[:, 0].max() + 0.1\n",
    "        x2_min, x2_max = x[:, 1].min() - 0.1, x[:, 1].max() + 0.1\n",
    "        xx1, xx2 = np.meshgrid(np.linspace(x1_min, x1_max, 100),\n",
    "                               np.linspace(x2_min, x2_max, 100))\n",
    "        grid_points = np.c_[xx1.ravel(), xx2.ravel()]\n",
    "\n",
    "        # Predict probabilities on the grid\n",
    "        proba_grid = predict_proba_func(params, grid_points)\n",
    "        proba_grid = np.array(proba_grid).reshape(xx1.shape)\n",
    "\n",
    "        # Plot probability contours (reversed colormap: red = high prob for class 1)\n",
    "        contour = ax.contourf(xx1, xx2, proba_grid, levels=20, cmap='RdYlBu_r', alpha=0.6)\n",
    "        cbar1 = plt.colorbar(contour, ax=ax)\n",
    "        cbar1.set_label('Forecasted probability p(y=1|x)', fontsize=10)\n",
    "\n",
    "        # Add decision boundary (p=0.5)\n",
    "        contour_lines = ax.contour(xx1, xx2, proba_grid, levels=[0.5], colors='black', \n",
    "                                   linewidths=2, linestyles='--', zorder=5)\n",
    "        ax.clabel(contour_lines, inline=True, fontsize=10, fmt='p=0.5')\n",
    "        \n",
    "        title = 'Training Data and Forecasted Probability'\n",
    "    else:\n",
    "        title = 'Training data in feature space x2'\n",
    "\n",
    "    # Plot training data points\n",
    "    scatter = ax.scatter(x[:, 0], x[:, 1], c=y, cmap='bwr', \n",
    "                         edgecolor='k', linewidth=1.5, s=50, alpha=0.9, zorder=10)\n",
    "    cbar2 = plt.colorbar(scatter, ax=ax, ticks=[0, 1], pad=0.15)\n",
    "    cbar2.set_label('True label y0', fontsize=10)\n",
    "\n",
    "    ax.set_xlabel('x1 (after sigmoid)', fontsize=11)\n",
    "    ax.set_ylabel('x2 (after sigmoid)', fontsize=11)\n",
    "    ax.set_title(title, fontsize=12, fontweight='bold')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot the training data\n",
    "plot_data_and_probability(x2, y0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa89f560",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248c5d08",
   "metadata": {},
   "source": [
    "### 2.1.1 Logistische Regression mit JAX\n",
    "\n",
    "Implementieren Sie das logistische Regressionsmodell und die negative Log-Likelihood-Verlustfunktion mit JAX. Stellen Sie sich JAX als Ersatz für NumPy mit automatischer Differentiation vor. Sie können den folgenden Code als Ausgangspunkt verwenden."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4f02fe",
   "metadata": {},
   "source": [
    "#### Parameter Initialisierung: `init_params`\n",
    "\n",
    "In dieser Funktion werden die Modellparemter $w$ und $b$ initialisiert. Die Gewichte $w$ werden aus einer Normalverteilung gezogen, während der Bias $b$ auf Null gesetzt wird."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd4b4d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# Parameter initialization (fully implemented)\n",
    "# ------------------------------------------------------------\n",
    "def init_params(seed=0):\n",
    "    \"\"\"\n",
    "    Initialize parameters for logistic regression.\n",
    "\n",
    "    Returns a dictionary:\n",
    "        params = {\n",
    "            \"w\": w,   # shape (2,)\n",
    "            \"b\": b    # scalar\n",
    "        }\n",
    "    \"\"\"\n",
    "    key = jax.random.PRNGKey(seed)\n",
    "    # Random initialization for weights (small values)\n",
    "    w = 0.1 * jax.random.normal(key, (2,))\n",
    "    # Bias initialized to zero\n",
    "    b = jnp.array(0.0)\n",
    "    return {\"w\": w, \"b\": b}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e171db1",
   "metadata": {},
   "source": [
    "> **Wichtig**: Durch verwendung eines festen Seeds erzeugt die Funktion bei jedem Aufruf die gleichen initialen Parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e459a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the parameter initialization\n",
    "params = init_params()\n",
    "print(\"Initialized parameters:\")\n",
    "print(\"w:\", params[\"w\"])\n",
    "print(\"b:\", params[\"b\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29dabc76",
   "metadata": {},
   "source": [
    "#### Logistische Regression: `predict_proba`\n",
    "\n",
    "Diese Funktion soll die Klassifikationswahrscheinlichkeit für die Klasse 1, also $P(y=1|x)$ für das logistische Regressionsmodell vorhersagen.\n",
    "\n",
    "- Eingabe:\n",
    "    - `params`: Ein Tupel `(W, b)`, wobei `W` die Gewichtsmatrix und `b` der Bias-Vektor ist.\n",
    "    - `x`: Ein Eingabedatenarray der Form `(n_samples, n_features)`.\n",
    "- Ausgabe:\n",
    "    - JAX-Array mit Wahrscheinlichkeiten im Bereich [0, 1] der Form `(n_samples,)` bei einem Batch, bzw. `(1,)` bei einem einzelnen Beispiel."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd5775c1",
   "metadata": {},
   "source": [
    "Als Formel für die logistische Regression gilt:\n",
    "$$P(y=1|x) = \\sigma(w^T x + b)$$\n",
    "mit \n",
    "$$\\sigma(z) = \\frac{1}{1 + e^{-z}}$$\n",
    "als Sigmoid-Funktion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b946fad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# Model: p(y = 1 | x) given the parameters\n",
    "# ------------------------------------------------------------\n",
    "def predict_proba(params, x):\n",
    "    \"\"\"\n",
    "    Compute p(y = 1 | x).\n",
    "    \n",
    "    #### Parameters:\n",
    "    - params: dict with model parameters \"w\" and \"b\"\n",
    "    - x: input features, shape (n_samples, 2)\n",
    "    #### Returns:\n",
    "    - p: predicted probabilities for class 1, shape (n_samples,)\n",
    "    \"\"\"\n",
    "    w = params[\"w\"]\n",
    "    b = params[\"b\"]\n",
    "    \n",
    "    # calculate linear score (logit)\n",
    "    z = jnp.dot(x, w) + b\n",
    "\n",
    "    # return sigmoid(z) = p\n",
    "    return jax.nn.sigmoid(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959d0fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test predict_proba\n",
    "params = init_params()\n",
    "x_test = jnp.array([[0.5, 0.5], [0.0, 0], [1.0, 1.0]])\n",
    "proba_test = predict_proba(params, x_test)\n",
    "print(f\"Test predict_proba input: {x_test.tolist()}\")\n",
    "print(\"Test predict_proba output:\", proba_test.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee92d6f",
   "metadata": {},
   "source": [
    "#### Kostenfunktion: `nll_loss`\n",
    "\n",
    "Diese Funktion berechnet die negative Log-Likelihood (NLL) für das logistische Regressionsmodell. Sie ist also die Kostenfunktion welche minimiert werden soll um $w$ und $b$ zu lernen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14e30fd",
   "metadata": {},
   "source": [
    "- Eingabe:\n",
    "    - `params`: Ein Tupel `(W, b)`, wobei `W` die Gewichtsmatrix und `b` der Bias-Vektor ist.\n",
    "    - `x`: Ein Eingabedatenarray der Form `(n_samples, n_features)`.\n",
    "    - `y`: Ein Array mit binären Zielwerten der Form `(n_samples,)`.\n",
    "- Ausgabe:\n",
    "    - Skalarwert des negativen Log-Likelihood-Verlusts.\n",
    "\n",
    "Formel für die negative Log-Likelihood-Verlustfunktion:\n",
    "$$\\text{L} = -\\frac{1}{N} \\sum_{i=1}^{N} \\left[ y_i \\log(p_i)) + (1 - y_i) \\log(1 - p_i) \\right]$$\n",
    "mit\n",
    "$$p_i = P(y=1|x_i) = \\sigma(w^T x_i + b)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3177bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# Negative Log-Likelihood loss\n",
    "# ------------------------------------------------------------\n",
    "def nll_loss(params, x, y, p_test=None, z_test = None):\n",
    "    \"\"\"\n",
    "    Binary cross-entropy / negative log-likelihood.\n",
    "\n",
    "    #### Parameters:\n",
    "    - params: dict with model parameters \"w\" and \"b\"\n",
    "    - x: input features, shape (n_samples, 2)\n",
    "    - y: true labels, shape (n_samples,)\n",
    "    #### Returns:\n",
    "    - loss: scalar, negative log-likelihood loss\n",
    "    \"\"\"\n",
    "    p = predict_proba(params, x) if p_test is None else p_test\n",
    "    eps = 1e-15  # small constant to avoid log(0)\n",
    "    w = params[\"w\"]\n",
    "    b = params[\"b\"]\n",
    "    z = jnp.dot(x, w) + b if z_test is None else z_test\n",
    "    #return -jnp.mean(y * jnp.log(p + eps) + (1 - y) * jnp.log(1 - p + eps))\n",
    "    return jnp.mean(jnp.logaddexp(0, z) - y * z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "942c3fad",
   "metadata": {},
   "source": [
    "> **Wichtig**: Für numerische Stabilität muss sichergestellt werden, dass die Argumente der Logarithmusfunktion nicht exakt 0 oder 1 sind da $\\log(0)$ undefiniert ist. Daher wird in der Funktion ein kleiner Wert `eps` verwendet, um die Argumente zu begrenzen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53ec7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test nll_loss\n",
    "params = init_params()\n",
    "loss_test = nll_loss(params, x2, y0)\n",
    "print(\"Test nll_loss:\", loss_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e5867a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\"w\": jnp.array([0.5, 0.5]), \"b\": jnp.array(0.0)}\n",
    "print(\"Initialized parameters:\")\n",
    "print(\"w:\", params[\"w\"])\n",
    "print(\"b:\", params[\"b\"])\n",
    "\n",
    "aL = 0.0179862106\n",
    "zL = -4.0\n",
    "\n",
    "x = jnp.array([[1.0, 1.0]])\n",
    "y = jnp.array([1.0])\n",
    "loss_test = nll_loss(params, x, y, aL, zL)\n",
    "print(\"Test nll_loss with custom params:\", loss_test)\n",
    "\n",
    "grad_f = jax.grad(nll_loss, argnums=0)\n",
    "grads = grad_f(params, x, y)\n",
    "print(\"Gradient w.r.t. w:\", grads[\"w\"])\n",
    "print(\"Gradient w.r.t. b:\", grads[\"b\"])\n",
    "grads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425035de",
   "metadata": {},
   "source": [
    "#### Trainingsfunktion: `train`\n",
    "\n",
    "Die Trainingsfunktion implementiert den Full-Batch-Gradientenabstieg zur Optimierung der Modellparameter $w$ und $b$.\n",
    "\n",
    "Die Funktion erstellt also ein Netzwerk mit einem einzigen Neuron Outputneuron (logistische Regression) und optimiert die Parameter mittels Gradientenabstieg basierend auf der negativen Log-Likelihood-Verlustfunktion.\n",
    "\n",
    "- Eingabe:\n",
    "    - `x`: Ein Eingabedatenarray der Form `(n_samples, n_features)`.\n",
    "    - `y`: Ein Array mit binären Zielwerten der Form `(n_samples,)`.\n",
    "    - `n_steps`: Anzahl der Trainingsschritte (Standard: 1000).\n",
    "    - `learning_rate`: Lernrate für den Gradientenabstieg (Standard: 0.1).\n",
    "- Ausgabe:\n",
    "    - Ein Tupel `(W, b)` mit den trainierten Modellparametern. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368f6dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# Full-batch gradient descent training loop\n",
    "# ------------------------------------------------------------\n",
    "def train(x, y, lr=0.5, n_steps=2000, init_seed=0):\n",
    "    \"\"\"\n",
    "    Full-batch gradient descent training.\n",
    "\n",
    "    TODO:\n",
    "      - Initialize params = init_params()\n",
    "      - Define grad_loss = jax.grad(nll_loss)\n",
    "      - Loop over steps:\n",
    "          * compute gradients\n",
    "          * update w and b\n",
    "      - Print loss every 200 steps (optional)\n",
    "      - Return final params\n",
    "    \"\"\"\n",
    "    # Initialize parameters\n",
    "    params = init_params(init_seed)\n",
    "\n",
    "    # Define gradient function using JAX\n",
    "    grad_loss = jax.grad(nll_loss)\n",
    "\n",
    "    for step in range(n_steps):\n",
    "        # Compute gradients\n",
    "        grads = grad_loss(params, x, y)\n",
    "\n",
    "        # Update parameters\n",
    "        params[\"w\"] -= lr * grads[\"w\"]\n",
    "        params[\"b\"] -= lr * grads[\"b\"]\n",
    "        \n",
    "        # Optional: Print loss every 200 steps\n",
    "        if step % 200 == 0:\n",
    "            current_loss = nll_loss(params, x, y)\n",
    "            print(f\"Step {step}, Loss: {current_loss:.4f}\")\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf25521b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# After students implement everything:\n",
    "params = train(x2, y0)\n",
    "# ------------------------------------------------------------\n",
    "# Plot the data and forecasted probability\n",
    "plot_data_and_probability(x2, y0, params, predict_proba)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7577a623",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f471d0",
   "metadata": {},
   "source": [
    "### 2.1.2 Multilayer Perceptron\n",
    "\n",
    "Fügen Sie dem Modell eine versteckte Ebene der Dimension 16 hinzu und wiederholen Sie [2.1.1](#211-logistische-regression-mit-jax). \n",
    "\n",
    "Um die Leistung zu verbessern, verwenden Sie jit, indem Sie den Aktualisierungsschritt in eine jit-Funktion einfügen.\n",
    "\n",
    "```python\n",
    "@jax.jit\n",
    "def update(params, x, y, lr):\n",
    "    grads = jax.grad(nll_loss)(params, x, y)\n",
    "    return {\n",
    "        \"W1\": params[\"W1\"] - lr * grads[\"W1\"],\n",
    "        # YOUR CODE HERE\n",
    "        }\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4380e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def update(params, x, y, lr):\n",
    "    grads = jax.grad(nll_loss)(params, x, y)\n",
    "    return {\n",
    "        \"W1\": params[\"W1\"] - lr * grads[\"W1\"],\n",
    "        \n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "028a3db3",
   "metadata": {},
   "source": [
    "In diesem Abschnitt implementieren wir ein kleines MLP (2 → 16 → 1) mit ReLU-Aktivierung, eine numerisch stabile NLL (BCE-with-logits) und einen jit-kompilierten Update-Schritt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0a2811",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "# ------------------------------\n",
    "# MLP Parameter-Initialisierung\n",
    "# ------------------------------\n",
    "def init_mlp_params(hidden_dim: int = 16, seed: int = 0):\n",
    "    key = jax.random.PRNGKey(seed)\n",
    "    k1, k2, k3 = jax.random.split(key, 3)\n",
    "    W1 = 0.1 * jax.random.normal(k1, (2, hidden_dim))\n",
    "    b1 = jnp.zeros((hidden_dim,))\n",
    "    W2 = 0.1 * jax.random.normal(k2, (hidden_dim, 1))\n",
    "    b2 = jnp.array(0.0)\n",
    "    return {\"W1\": W1, \"b1\": b1, \"W2\": W2, \"b2\": b2}\n",
    "\n",
    "# ------------------------------\n",
    "# MLP Forward (Logits & Proba)\n",
    "# ------------------------------\n",
    "def mlp_logits(params, x):\n",
    "    x = jnp.asarray(x)\n",
    "    if x.ndim == 1:\n",
    "        x = x[None, :]\n",
    "    h = jax.nn.relu(jnp.dot(x, params[\"W1\"]) + params[\"b1\"])  # (N, 16)\n",
    "    z = jnp.dot(h, params[\"W2\"]) + params[\"b2\"]               # (N, 1)\n",
    "    return z.squeeze(-1)                                        # (N,) oder ()\n",
    "\n",
    "def mlp_predict_proba(params, x):\n",
    "    return jax.nn.sigmoid(mlp_logits(params, x))\n",
    "\n",
    "# -------------------------------------\n",
    "# Numerisch stabile NLL (BCE-with-logits)\n",
    "# -------------------------------------\n",
    "def nll_loss_mlp(params, x, y):\n",
    "    y = jnp.asarray(y)\n",
    "    z = mlp_logits(params, x)\n",
    "    return jnp.mean(jnp.logaddexp(0.0, z) - y * z)\n",
    "\n",
    "# ------------------------------\n",
    "# jit-kompiliertes Update\n",
    "# ------------------------------\n",
    "@jax.jit\n",
    "def update(params, x, y, lr):\n",
    "    grads = jax.grad(nll_loss_mlp)(params, x, y)\n",
    "    return {\n",
    "        \"W1\": params[\"W1\"] - lr * grads[\"W1\"],\n",
    "        \"b1\": params[\"b1\"] - lr * grads[\"b1\"],\n",
    "        \"W2\": params[\"W2\"] - lr * grads[\"W2\"],\n",
    "        \"b2\": params[\"b2\"] - lr * grads[\"b2\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4efaed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kleiner Trainingslauf für das MLP\n",
    "\n",
    "def train_mlp(x, y, lr=0.05, n_steps=2000, seed=0):\n",
    "    params = init_mlp_params(hidden_dim=16, seed=seed)\n",
    "    for step in range(n_steps):\n",
    "        params = update(params, x, y, lr)\n",
    "        if step % 200 == 0:\n",
    "            current_loss = nll_loss_mlp(params, x, y)\n",
    "            print(f\"Step {step}, MLP Loss: {current_loss:.4f}\")\n",
    "    return params\n",
    "\n",
    "# Trainieren und Visualisieren\n",
    "params_mlp = train_mlp(x2, y0, lr=0.05, n_steps=2000, seed=0)\n",
    "plot_data_and_probability(x2, y0, params_mlp, mlp_predict_proba)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16399b60",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a6dcef0",
   "metadata": {},
   "source": [
    "### 2.1.3 Implementieren Sie das MLP in Keras.\n",
    "\n",
    "Implementieren Sie das MLP in Keras unter Verwendung der sequenziellen API."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93747192",
   "metadata": {},
   "source": [
    "Wir implementieren ein kleines MLP mit der Keras-Sequential-API (2 → 16 → 1), kompilieren mit Adam und Binary Cross-Entropy und trainieren es auf den vorhandenen Daten `x2, y0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41697f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras import layers, Sequential, optimizers, losses\n",
    "\n",
    "# Optional: Reproduzierbarkeit\n",
    "try:\n",
    "    keras.utils.set_random_seed(0)\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# Modell definieren\n",
    "model = Sequential([\n",
    "    layers.Input(shape=(2,)),\n",
    "    layers.Dense(16, activation='relu'),\n",
    "    layers.Dense(16, activation='relu'),\n",
    "    layers.Dense(8, activation='relu'),\n",
    "    layers.Dense(1, activation='sigmoid'),\n",
    "])\n",
    "\n",
    "# Kompilieren\n",
    "model.compile(\n",
    "    optimizer=optimizers.Adam(learning_rate=0.01),\n",
    "    loss=losses.BinaryCrossentropy(),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Trainieren\n",
    "history = model.fit(\n",
    "    x2, y0,\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    verbose=0,\n",
    "    validation_split=0.2\n",
    ")\n",
    "print(f\"Final train acc: {history.history['accuracy'][-1]:.4f}\")\n",
    "print(f\"Final val acc:   {history.history['val_accuracy'][-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d2dd2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Keras-Proba-Wrapper für die bestehende Plot-Funktion\n",
    "\n",
    "def predict_proba_keras(model, x):\n",
    "    preds = model.predict(np.asarray(x), verbose=0)\n",
    "    return preds.ravel()\n",
    "\n",
    "# Visualisierung mit der vorhandenen Utility\n",
    "plot_data_and_probability(x2, y0, model, predict_proba_keras)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
